apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: ">= v0.15.0"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: monitoring
      interval: 1m
  values:
    alertmanager:
      ingress:
        enabled: false
        ingressClassName: nginx
        annotations:
          cert-manager.io/cluster-issuer: homelab-internal
        hosts:
          - alertmanager.example.com
        tls:
          - hosts:
              - alertmanager.example.com
            secretName: alertmanager-tls


    grafana:
      defaultDashboardsTimezone: America/Toronto
      ingress:
        enabled: false
        ingressClassName: nginx
        annotations:
          cert-manager.io/cluster-issuer: homelab-internal
        hosts:
          - grafana.example.com
        tls:
          - hosts:
              - grafana.example.com
            secretName: grafana-tls
      persistence:
        enabled: true
        storageClassName: "rook-ceph-retain"
        accessModes:
          - ReadWriteOnce
        size: 20Gi
      ## Configure grafana dashboard providers
      ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
      ##
      ## `path` must be /var/lib/grafana/dashboards/<provider_name>
      ##
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
        - name: 'traefik'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/traefik
      #  dashboardproviders.yaml:
      #    apiVersion: 1
      #    providers:
      #    - name: 'default'
      #      orgId: 1
      #      folder: ''
      #      type: file
      #      disableDeletion: false
      #      editable: true
      #      options:
      #        path: /var/lib/grafana/dashboards/default

      ## Configure grafana dashboard to import
      ## NOTE: To use dashboards you must also enable/configure dashboardProviders
      ## ref: https://grafana.com/dashboards
      ##
      ## dashboards per provider, use provider name as key.
      ##
      dashboards:
        traefik:
          official-dashboard:
            url: https://raw.githubusercontent.com/traefik/traefik/refs/heads/master/contrib/grafana/traefik.json
        # default:
        #   some-dashboard:
        #     json: |
        #       $RAW_JSON
        #   custom-dashboard:
        #     file: dashboards/custom-dashboard.json
        #   prometheus-stats:
        #     gnetId: 2
        #     revision: 2
        #     datasource: Prometheus
        #   local-dashboard:
        #     url: https://example.com/repository/test.json
        #     token: ''
        #   local-dashboard-base64:
        #     url: https://example.com/repository/test-b64.json
        #     token: ''
        #     b64content: true
        #   local-dashboard-gitlab:
        #     url: https://example.com/repository/test-gitlab.json
        #     gitlabToken: ''
        #   local-dashboard-bitbucket:
        #     url: https://example.com/repository/test-bitbucket.json
        #     bearerToken: ''
        #   local-dashboard-azure:
        #     url: https://example.com/repository/test-azure.json
        #     basic: ''
        #     acceptHeader: '*/*'

      ## Reference to external ConfigMap per provider. Use provider name as key and ConfigMap name as value.
      ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.
      ## ConfigMap data example:
      ##
      ## data:
      ##   example-dashboard.json: |
      ##     RAW_JSON
      ##
      dashboardsConfigMaps: {}
      #  default: ""


    kubeControllerManager:
      enabled: true
      service:
        selector:
          k8s-app: kube-controller-manager


    kubeEtcd:
      enabled: true
      service:
        selector:
          # Fix selector for kube-etcd for Talos
          # (set itentionally to kube-controller-manager because all master nodes has the same roles)
          k8s-app: kube-controller-manager
      serviceMonitor:
        # Add nodename label
        relabelings:
          - sourceLabels: [__meta_kubernetes_pod_node_name]
            separator: ;
            regex: ^(.*)$
            targetLabel: nodename
            replacement: $1
            action: replace
        # Remove pod label
        metricRelabelings:
          - action: labeldrop
            regex: pod


    kubeScheduler:
      enabled: true
      service:
        selector:
          k8s-app: kube-controller-manager


    prometheusOperator:
      admissionWebhooks:
        certManager:
          enabled: true


    prometheus:
      ingress:
        enabled: false
        ingressClassName: nginx
        annotations:
          cert-manager.io/cluster-issuer: homelab-internal
        hosts:
          - example.com
        tls:
          - hosts:
              - example.com
            secretName: prometheus-tls
      prometheusSpec:
        ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
        ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
        ## as specified in the official Prometheus documentation:
        ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
        ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
        ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
        ## scrape configs are going to break Prometheus after the upgrade.
        ## AdditionalScrapeConfigs can be defined as a list or as a templated string.
        ##
        ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
        ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
        ##
        # additionalScrapeConfigs:
        #   - job_name: 'traefik'
        #     static_configs:
        #       - targets:
        #           # - traefik.svc.home.enoxi.me:8082
        #           - 192.168.70.5:8082
        # - job_name: kube-etcd
        #   kubernetes_sd_configs:
        #     - role: node
        #   scheme: https
        #   tls_config:
        #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
        #   relabel_configs:
        #   - action: labelmap
        #     regex: __meta_kubernetes_node_label_(.+)
        #   - source_labels: [__address__]
        #     action: replace
        #     targetLabel: __address__
        #     regex: ([^:;]+):(\d+)
        #     replacement: ${1}:2379
        #   - source_labels: [__meta_kubernetes_node_name]
        #     action: keep
        #     regex: .*mst.*
        #   - source_labels: [__meta_kubernetes_node_name]
        #     action: replace
        #     targetLabel: node
        #     regex: (.*)
        #     replacement: ${1}
        #   metric_relabel_configs:
        #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)
        #     action: labeldrop
        #
        ## If scrape config contains a repetitive section, you may want to use a template.
        ## In the following example, you can see how to define `gce_sd_configs` for multiple zones
        # additionalScrapeConfigs: |
        #  - job_name: "node-exporter"
        #    gce_sd_configs:
        #    {{range $zone := .Values.gcp_zones}}
        #    - project: "project1"
        #      zone: "{{$zone}}"
        #      port: 9100
        #    {{end}}
        #    relabel_configs:
        #    ...


        ## If additional scrape configurations are already deployed in a single secret file you can use this section.
        ## Expected values are the secret name and key
        ## Cannot be used with additionalScrapeConfigs
        additionalScrapeConfigsSecret: {}
          # enabled: false
          # name:
          # key:
